import tweepy
import pickle
import gpt_2_simple as gpt2
from datetime import datetime
import json
import random as rand
import time
import numpy as np
import re
import os


# TensorFlow quiet loading
os.environ["TF_CPP_MIN_LOG_LEVEL"] = '3'


# Load API keys for Twitter OAuth
with open('tokens.json') as f:
  keys = json.load(f)
 
 
# Authenticate with Twitter
auth = tweepy.OAuthHandler(keys["CONSUMER_KEY"], keys["CONSUMER_SECRET"])
auth.set_access_token(keys["ACCESS_TOKEN"], keys["ACCESS_TOKEN_SECRET"])
# Create API object
api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)


# Load scraped text
fname = "data/letters_punct.pkl"
with open(fname, 'rb') as f:
    processed_letters = pickle.load(f)


# Get some sentence starters from the raw (scraped) text
phrase_starters = []
for letter in processed_letters:
    paragraphs = re.split("\n", letter)
    for paragraph in paragraphs:
        words = paragraph.split(" ")
        phrase = " ".join(words[0:5])
        phrase = re.compile("\\s+$|^\\s+").sub("", phrase)
        if not re.findall('^"', phrase):
          phrase_starters.append(phrase)
        

# Load pre-trained model from local storage
sess = gpt2.start_tf_sess()
gpt2.load_gpt2(sess)


# Create function that generates new text (with parameters) and updates Twitter status
def new_status(api:tweepy.api, word_count:int, temperature:float, prefix=True):
  """Generates a Twitter status update, using text generated by a trained GPT-2 model."""
  # Output file name
  output_file = 'text/gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())
  # Topic prefix
  if prefix and not isinstance(prefix, str):
    prefix = phrase_starters[rand.randint(0, len(phrase_starters))]
  else:
    assert isinstance(prefix, str)
  gpt2.generate_to_file(
                  sess,
                  destination_path = output_file,
                  length=word_count*1.25,
                  temperature=temperature,
                  prefix=prefix,
                  # truncate=".",
                  nsamples=1,
                  batch_size=1
                  )
  # Read text from generated file
  with open(output_file, "r") as f:
    generated_text = f.readlines()
  # Concatenate and remove line breaks
  generated_text = (" ".join(generated_text)).replace("\n"," ")
  # Split text into a list of sentences
  generated_sentences = re.split('[\\.\\?!]\\s', generated_text)
  # Calculate the length of each sentence (add one for punctuation at the end of sentences)
  sentence_lengths = [len(sentence)+1 for sentence in generated_sentences]
  # Sample sentences (meeting minimum threshold length) at random
  indices = np.where(np.array(sentence_lengths) > 200)
  selected_index = np.random.choice(indices[0])
  sentence = generated_sentences[selected_index]
  # Split string by word
  words = sentence.split(" ")
  # Containers
  status_text = []
  text = ""
  # Sequentially adds words to a string until they exceed word limit
  while words:
    if (len(text) + len(words[0]) + 1) <  274:
      text = text + " " + words[0]
      del words[0]
    else:
      status_text.append(text)
      text = ""
  status_text.append(text)
  # How many tweets to spread text over?
  num_tweets = len(status_text)
  for tweet in range(0, num_tweets):
    if num_tweets > 1:
      text = status_text[tweet][1:]
      # Create a status update (with sequence formatting)
      status = text + " <{}/{}>".format(tweet+1, num_tweets)
      if tweet == 0:
        # Status update to timeline
        api.update_status(status)
        tweetId = api.get_user("SenecaGPT2").status.id
      else:
        # Create status update as a reply in thread
        api.update_status(status, in_reply_to_status_id = tweetId)
        tweetId = api.get_user("SenecaGPT2").status.id
    else:
      # Create a status update (without sequence formatting)
      status = status_text[tweet][1:]
      api.update_status(status)
    print(status)


# Create wrapper function for the `new_status` function, used when scheduling status updates
def run_task():
  """Wrapper for the new_status() function defined above. Needed to modify options and schedule tasks."""  
  rng = rand.uniform(0, 1)
  if rng < 0.9: #  Uses pre-defined phrase starters 90% of the time
    prefix = phrase_starters[rand.randint(0, len(phrase_starters))]
  else:
    # The other 10% are to be based on trending topics
    USA_WOE_ID = 23424977
    usa_trends = api.trends_place(USA_WOE_ID)
    trends = json.loads(json.dumps(usa_trends, indent=1))
    choices = []
    for trend in trends[0]["trends"]:
      topic = re.findall("\\w+\\s\\w+|^#\\w+", trend["name"])
      if topic:
        choices.append(topic[0]+" ")
    # Randomly select starting phrase (prefix)
    prefix = choices[rand.randint(0, len(choices))]
  # Randomize word count
  word_count = rand.randint(350, 750)
  # Vary the temperature parameter
  temperature = rand.uniform(0.67, 0.72)
  # Specify options for the new status update
  opts = {"api":api, "word_count":word_count, "temperature":temperature}
  new_status(**opts)
    
    
# Schedule status updates at Gamma distributed time intervals
while True:
  delta = rand.gammavariate(alpha=1e2, beta=1e-1) # mean (hours) = alpha*beta
  time_to_run = time.time() + (delta * 3600)
  if time.time() > time_to_run:
    run_task()
  else:
    time.sleep(10)

